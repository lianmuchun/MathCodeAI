import streamlit as st  # å¯¼å…¥ Streamlit åº“ï¼Œç”¨äºåˆ›å»º Web åº”ç”¨ç•Œé¢
import logging  # å¯¼å…¥ logging åº“ï¼Œç”¨äºè®°å½•æ—¥å¿—
import os  # å¯¼å…¥ os åº“ï¼Œç”¨äºæ“ä½œæ–‡ä»¶å’Œè·¯å¾„
import tempfile  # å¯¼å…¥ tempfile åº“ï¼Œç”¨äºåˆ›å»ºä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
import shutil  # å¯¼å…¥ shutil åº“ï¼Œç”¨äºæ–‡ä»¶æ“ä½œï¼Œå¦‚å¤åˆ¶å’Œåˆ é™¤
import pdfplumber  # å¯¼å…¥ pdfplumber åº“ï¼Œç”¨äºå¤„ç† PDF æ–‡ä»¶
import ollama  # å¯¼å…¥ ollama åº“ï¼Œç”¨äº Ollama æ¨¡å‹çš„æ¥å£

# å¯¼å…¥ LangChain ç›¸å…³æ¨¡å—ï¼Œç”¨äºå¤„ç†æ–‡æ¡£åŠ è½½ã€åµŒå…¥ç”Ÿæˆã€æ–‡æœ¬åˆ†å‰²ã€å‘é‡å­˜å‚¨ç­‰åŠŸèƒ½
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
from typing import List, Tuple, Dict, Any, Optional  # å¯¼å…¥ typing æ¨¡å—ï¼Œç”¨äºç±»å‹æ³¨é‡Š

# Streamlit é¡µé¢é…ç½®
st.set_page_config(
    page_title="MathCode AI",  # é¡µé¢æ ‡é¢˜
    page_icon="ğŸš€",  # é¡µé¢å›¾æ ‡
    layout="wide",  # é¡µé¢å¸ƒå±€ä¸ºå®½å±
    initial_sidebar_state="collapsed",  # ä¾§è¾¹æ é»˜è®¤çŠ¶æ€ä¸ºæ”¶èµ·
)

with st.sidebar:
    st.markdown("# ğŸ’¡ å…³äº")
    st.divider()
    st.markdown("**æ¶æ„âš™ï¸**ï¼š*Ollama + Streamlit + LangChain*")
    st.markdown("**å›¢é˜ŸğŸ¥‡**ï¼šæ•°ç å®è´")
    st.markdown("**ä½œè€…ğŸ‘¦ğŸ»**ï¼šææ™ºç› & åˆ˜çº¢æ–Œ & å¢å…´æ¹› & æ›¹ä¿Šæ³½ & å‘¨æµå¤")
    st.markdown("**ä½œè€…ğŸ‘©ğŸ»**ï¼šæ¢æ€æ° & å‘¨æ´")
    st.markdown("**ä»£ç ä»“åº“ğŸ’»**ï¼š*https://gitee.com/liang-sijie/code-babies*")

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,  # æ—¥å¿—çº§åˆ«ä¸º INFO
    format="%(asctime)s - %(levelname)s - %(message)s",  # æ—¥å¿—æ ¼å¼
    datefmt="%Y-%m-%d %H:%M:%S",  # æ—¥æœŸæ ¼å¼
)

logger = logging.getLogger(__name__)  # è·å–æ—¥å¿—è®°å½•å™¨

# ç¼“å­˜å‡½æ•°ï¼Œç”¨äºæå–æ¨¡å‹åç§°
@st.cache_resource(show_spinner=True)
def extract_model_names(
    models_info: Dict[str, List[Dict[str, Any]]],  # è¾“å…¥å‚æ•°ï¼šæ¨¡å‹ä¿¡æ¯å­—å…¸
) -> Tuple[str, ...]:

    logger.info("Extracting model names from models_info")  # è®°å½•æå–æ¨¡å‹åç§°çš„æ—¥å¿—
    model_names = tuple(model["name"] for model in models_info["models"])  # æå–æ¨¡å‹åç§°
    logger.info(f"Extracted model names: {model_names}")  # è®°å½•æå–çš„æ¨¡å‹åç§°
    return model_names  # è¿”å›æ¨¡å‹åç§°å…ƒç»„

# åˆ›å»ºå‘é‡æ•°æ®åº“çš„å‡½æ•°
def create_vector_db(file_upload) -> Chroma:

    logger.info(f"Creating vector DB from file upload: {file_upload.name}")  # è®°å½•åˆ›å»ºå‘é‡æ•°æ®åº“çš„æ—¥å¿—
    temp_dir = tempfile.mkdtemp()  # åˆ›å»ºä¸´æ—¶ç›®å½•

    path = os.path.join(temp_dir, file_upload.name)  # å®šä¹‰ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    with open(path, "wb") as f:  # æ‰“å¼€ä¸´æ—¶æ–‡ä»¶è¿›è¡Œå†™æ“ä½œ
        f.write(file_upload.getvalue())  # å°†ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹å†™å…¥ä¸´æ—¶æ–‡ä»¶
        logger.info(f"File saved to temporary path: {path}")  # è®°å½•æ–‡ä»¶ä¿å­˜è·¯å¾„
        loader = UnstructuredPDFLoader(path)  # åˆ›å»º PDF åŠ è½½å™¨
        data = loader.load()  # åŠ è½½ PDF å†…å®¹

    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨ï¼Œå°†æ–‡æ¡£åˆ†å‰²æˆå—
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
    chunks = text_splitter.split_documents(data)  # å°†æ–‡æ¡£åˆ†å‰²æˆå—
    logger.info("Document split into chunks")  # è®°å½•æ–‡æ¡£åˆ†å‰²å®Œæˆçš„æ—¥å¿—

    # åˆ›å»º Ollama åµŒå…¥ç”Ÿæˆå™¨
    embeddings = OllamaEmbeddings(model="nomic-embed-text:v1.5", show_progress=True)
    vector_db = Chroma.from_documents(
        documents=chunks, embedding=embeddings, collection_name="myRAG"  # åˆ›å»ºå‘é‡æ•°æ®åº“
    )
    logger.info("Vector DB created")  # è®°å½•å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆçš„æ—¥å¿—

    shutil.rmtree(temp_dir)  # åˆ é™¤ä¸´æ—¶ç›®å½•
    logger.info(f"Temporary directory {temp_dir} removed")  # è®°å½•ä¸´æ—¶ç›®å½•åˆ é™¤çš„æ—¥å¿—
    return vector_db  # è¿”å›å‘é‡æ•°æ®åº“å¯¹è±¡

# å¤„ç†ç”¨æˆ·é—®é¢˜çš„å‡½æ•°
def process_question(question: str, vector_db: Chroma, selected_model: str) -> str:

    logger.info(f"Processing question: {question} using model: {selected_model}")  # è®°å½•å¤„ç†é—®é¢˜çš„æ—¥å¿—
    llm = ChatOllama(model=selected_model, temperature=0)  # åˆ›å»ºèŠå¤©æ¨¡å‹å®ä¾‹
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],  # è¾“å…¥å˜é‡
        template="""You are an AI language model assistant. Your task is to generate 3
        different versions of the given user question to retrieve relevant documents from
        a vector database. By generating multiple perspectives on the user question, your
        goal is to help the user overcome some of the limitations of the distance-based
        similarity search. Provide these alternative questions separated by newlines.
        Original question: {question}""",  # æé—®æç¤ºæ¨¡æ¿
    )

    # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢å™¨
    retriever = MultiQueryRetriever.from_llm(
        vector_db.as_retriever(), llm, prompt=QUERY_PROMPT
    )

    template = """Answer the question based ONLY on the following context:
    {context}
    Question: {question}
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Only provide the answer from the {context}, nothing else.
    Add snippets of the context you used to answer the question.
    """

    prompt = ChatPromptTemplate.from_template(template)  # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿

    # åˆ›å»ºå¤„ç†é“¾ï¼Œå°†ä¸Šä¸‹æ–‡ã€é—®é¢˜ã€æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨è¿æ¥èµ·æ¥
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    response = chain.invoke(question)  # æ‰§è¡Œå¤„ç†é“¾ï¼Œç”Ÿæˆå›ç­”
    logger.info("Question processed and response generated")  # è®°å½•é—®é¢˜å¤„ç†å®Œæˆçš„æ—¥å¿—
    return response  # è¿”å›ç”Ÿæˆçš„å›ç­”

# ç¼“å­˜å‡½æ•°ï¼Œç”¨äºæå– PDF æ–‡ä»¶çš„æ‰€æœ‰é¡µé¢ä½œä¸ºå›¾åƒ
@st.cache_data
def extract_all_pages_as_images(file_upload) -> List[Any]:

    logger.info(f"Extracting all pages as images from file: {file_upload.name}")  # è®°å½•æå– PDF é¡µé¢çš„æ—¥å¿—
    pdf_pages = []  # åˆå§‹åŒ–é¡µé¢åˆ—è¡¨
    with pdfplumber.open(file_upload) as pdf:  # æ‰“å¼€ PDF æ–‡ä»¶
        pdf_pages = [page.to_image().original for page in pdf.pages]  # å°†æ¯ä¸ªé¡µé¢æå–ä¸ºå›¾åƒ
    logger.info("PDF pages extracted as images")  # è®°å½•é¡µé¢æå–å®Œæˆçš„æ—¥å¿—
    return pdf_pages  # è¿”å›é¡µé¢å›¾åƒåˆ—è¡¨

# åˆ é™¤å‘é‡æ•°æ®åº“çš„å‡½æ•°
def delete_vector_db(vector_db: Optional[Chroma]) -> None:

    logger.info("Deleting vector DB")  # è®°å½•åˆ é™¤å‘é‡æ•°æ®åº“çš„æ—¥å¿—
    if vector_db is not None:
        vector_db.delete_collection()  # åˆ é™¤å‘é‡æ•°æ®åº“é›†åˆ
        st.session_state.pop("pdf_pages", None)  # æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„ PDF é¡µé¢æ•°æ®
        st.session_state.pop("file_upload", None)  # æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„æ–‡ä»¶ä¸Šä¼ æ•°æ®
        st.session_state.pop("vector_db", None)  # æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„å‘é‡æ•°æ®åº“
        st.success("Collection and temporary files deleted successfully.")  # æ˜¾ç¤ºåˆ é™¤æˆåŠŸæ¶ˆæ¯
        logger.info("Vector DB and related session state cleared")  # è®°å½•å‘é‡æ•°æ®åº“åˆ é™¤å®Œæˆçš„æ—¥å¿—
        st.rerun()  # é‡æ–°è¿è¡Œåº”ç”¨ç¨‹åº
    else:
        st.error("No vector database found to delete.")  # æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯ï¼Œæç¤ºæœªæ‰¾åˆ°å‘é‡æ•°æ®åº“
        logger.warning("Attempted to delete vector DB, but none was found")  # è®°å½•æœªæ‰¾åˆ°å‘é‡æ•°æ®åº“çš„è­¦å‘Šæ—¥å¿—

# ä¸»å‡½æ•°ï¼Œç”¨äºè¿è¡Œ Streamlit åº”ç”¨ç¨‹åº
def main() -> None:

    st.title("ğŸš€ MathCode AI")
    st.subheader("", divider="gray", anchor=False)  # è®¾ç½®åº”ç”¨æ ‡é¢˜

    models_info = ollama.list()  # è·å–å¯ç”¨æ¨¡å‹çš„ä¿¡æ¯
    available_models = extract_model_names(models_info)  # æå–æ¨¡å‹åç§°

    col1, col2 = st.columns([1.5, 2])  # åˆ›å»ºä¸¤ä¸ªåˆ—å¸ƒå±€

    if "messages" not in st.session_state:  # å¦‚æœä¼šè¯çŠ¶æ€ä¸­æ²¡æœ‰æ¶ˆæ¯ï¼Œåˆ™åˆå§‹åŒ–
        st.session_state["messages"] = []

    if "vector_db" not in st.session_state:  # å¦‚æœä¼šè¯çŠ¶æ€ä¸­æ²¡æœ‰å‘é‡æ•°æ®åº“ï¼Œåˆ™åˆå§‹åŒ–
        st.session_state["vector_db"] = None

    if available_models:  # å¦‚æœæœ‰å¯ç”¨æ¨¡å‹
        selected_model = col2.selectbox(
            "ğŸŒˆ **é€‰æ‹©æ¨¡å‹**", available_models  # åœ¨åˆ— 2 ä¸­åˆ›å»ºæ¨¡å‹é€‰æ‹©æ¡†
        )

    file_upload = col1.file_uploader(
        "ğŸ“ **ä¸Šä¼  PDF æ–‡ä»¶**", type="pdf", accept_multiple_files=False  # åœ¨åˆ— 1 ä¸­åˆ›å»ºæ–‡ä»¶ä¸Šä¼ å™¨
    )

    if file_upload:  # å¦‚æœæ–‡ä»¶å·²ä¸Šä¼ 
        st.session_state["file_upload"] = file_upload  # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¼šè¯çŠ¶æ€
        if st.session_state["vector_db"] is None:  # å¦‚æœå‘é‡æ•°æ®åº“æœªåˆ›å»º
            st.session_state["vector_db"] = create_vector_db(file_upload)  # åˆ›å»ºå‘é‡æ•°æ®åº“
        pdf_pages = extract_all_pages_as_images(file_upload)  # æå– PDF é¡µé¢ä¸ºå›¾åƒ
        st.session_state["pdf_pages"] = pdf_pages  # ä¿å­˜é¡µé¢å›¾åƒåˆ°ä¼šè¯çŠ¶æ€

        zoom_level = col1.slider(
            "Zoom Level", min_value=100, max_value=1000, value=700, step=50  # åœ¨åˆ— 1 ä¸­åˆ›å»ºç¼©æ”¾çº§åˆ«æ»‘å—
        )

        with col1:  # åœ¨åˆ— 1 ä¸­æ˜¾ç¤º PDF é¡µé¢å›¾åƒ
            with st.container(height=410, border=True):
                for page_image in pdf_pages:  # éå†æ¯ä¸ªé¡µé¢å›¾åƒ
                    st.image(page_image, width=zoom_level)  # æ˜¾ç¤ºå›¾åƒï¼Œä½¿ç”¨ç¼©æ”¾çº§åˆ«

    delete_collection = col1.button("âš ï¸ **åˆ é™¤ä¼šè¯**", type="secondary")  # åœ¨åˆ— 1 ä¸­åˆ›å»ºåˆ é™¤é›†åˆæŒ‰é’®

    if delete_collection:  # å¦‚æœæŒ‰ä¸‹åˆ é™¤æŒ‰é’®
        delete_vector_db(st.session_state["vector_db"])  # åˆ é™¤å‘é‡æ•°æ®åº“

    with col2:  # åœ¨åˆ— 2 ä¸­æ˜¾ç¤ºæ¶ˆæ¯
        message_container = st.container(height=500, border=True)

        for message in st.session_state["messages"]:  # éå†ä¼šè¯çŠ¶æ€ä¸­çš„æ¶ˆæ¯
            avatar = "ğŸ¤–" if message["role"] == "assistant" else "ğŸ˜"  # æ ¹æ®æ¶ˆæ¯è§’è‰²è®¾ç½®å¤´åƒ
            with message_container.chat_message(message["role"], avatar=avatar):  # åœ¨æ¶ˆæ¯å®¹å™¨ä¸­æ˜¾ç¤ºæ¶ˆæ¯
                st.markdown(message["content"])  # æ˜¾ç¤ºæ¶ˆæ¯å†…å®¹

        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):  # å¦‚æœç”¨æˆ·è¾“å…¥äº†æç¤º
            try:
                st.session_state["messages"].append({"role": "user", "content": prompt})  # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€
                message_container.chat_message("user", avatar="ğŸ˜").markdown(prompt)  # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯

                with message_container.chat_message("assistant", avatar="ğŸ¤–"):  # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
                    with st.spinner(":green[processing...]"):  # æ˜¾ç¤ºå¤„ç†ä¸­çš„æç¤º
                        if st.session_state["vector_db"] is not None:  # å¦‚æœå‘é‡æ•°æ®åº“å·²åˆ›å»º
                            response = process_question(
                                prompt, st.session_state["vector_db"], selected_model  # å¤„ç†ç”¨æˆ·é—®é¢˜
                            )
                            st.markdown(response)  # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
                        else:
                            st.warning("è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶")  # æç¤ºç”¨æˆ·å…ˆä¸Šä¼  PDF æ–‡ä»¶

                if st.session_state["vector_db"] is not None:  # å¦‚æœå‘é‡æ•°æ®åº“å·²åˆ›å»º
                    st.session_state["messages"].append(
                        {"role": "assistant", "content": response}  # å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€
                    )

            except Exception as e:  # æ•è·å¼‚å¸¸
                st.error(e, icon="â›”ï¸")  # æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯
                logger.error(f"Error processing prompt: {e}")  # è®°å½•é”™è¯¯æ—¥å¿—
        else:
            if st.session_state["vector_db"] is None:  # å¦‚æœæœªä¸Šä¼  PDF æ–‡ä»¶
                st.warning("å¼€å§‹èŠå¤©å‰è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶...")  # æç¤ºç”¨æˆ·ä¸Šä¼  PDF æ–‡ä»¶

# ç¨‹åºå…¥å£ï¼Œè¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    main()